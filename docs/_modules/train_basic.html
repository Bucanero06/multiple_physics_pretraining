<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>train_basic &mdash; Multiple Physics Pretrained 0.0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=fd3f3429" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=d45e8c67"></script>
        <script src="../_static/doctools.js?v=9a2dae69"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Multiple Physics Pretrained
          </a>
              <div class="version">
                0.0.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../autodocumentation_python.html">autodocumentation_python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data_utils.datasets.html">data_utils.datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data_utils.hdf5_datasets.html">data_utils.hdf5_datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data_utils.mixed_dset_sampler.html">data_utils.mixed_dset_sampler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data_utils.pdebench_download_utils.download_pdebench_data.html">data_utils.pdebench_download_utils.download_pdebench_data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../models.avit.html">models.avit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../models.mixed_modules.html">models.mixed_modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../models.shared_modules.html">models.shared_modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../models.spatial_modules.html">models.spatial_modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../models.time_modules.html">models.time_modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../shared.command_execution.html">shared.command_execution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../shared.config.html">shared.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../shared.configuration_parser.html">shared.configuration_parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="../shared.data_processing_utils.html">shared.data_processing_utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../shared.file_directory_ops.html">shared.file_directory_ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../shared.logging.html">shared.logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../shared.string_dict_utils.html">shared.string_dict_utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../shared.tmp_shared.html">shared.tmp_shared</a></li>
<li class="toctree-l1"><a class="reference internal" href="../train_basic.html">train_basic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils.YParams.html">utils.YParams</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils.logging_utils.html">utils.logging_utils</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Multiple Physics Pretrained</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Module code</a></li>
      <li class="breadcrumb-item active">train_basic</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for train_basic</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.cuda.amp</span> <span class="k">as</span> <span class="nn">amp</span>
<span class="kn">from</span> <span class="nn">torch.nn.parallel</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span>
<span class="kn">from</span> <span class="nn">einops</span> <span class="kn">import</span> <span class="n">rearrange</span>
<span class="kn">from</span> <span class="nn">ruamel.yaml</span> <span class="kn">import</span> <span class="n">YAML</span>
<span class="kn">from</span> <span class="nn">ruamel.yaml.comments</span> <span class="kn">import</span> <span class="n">CommentedMap</span> <span class="k">as</span> <span class="n">ruamelDict</span>
<span class="kn">from</span> <span class="nn">dadaptation</span> <span class="kn">import</span> <span class="n">DAdaptAdam</span><span class="p">,</span> <span class="n">DAdaptAdan</span>
<span class="kn">from</span> <span class="nn">adan_pytorch</span> <span class="kn">import</span> <span class="n">Adan</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="kn">import</span> <span class="nn">wandb</span>
<span class="kn">import</span> <span class="nn">pickle</span> <span class="k">as</span> <span class="nn">pkl</span>
<span class="kn">import</span> <span class="nn">gc</span>
<span class="kn">from</span> <span class="nn">torchinfo</span> <span class="kn">import</span> <span class="n">summary</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="kn">from</span> <span class="nn">torch.nn.parallel</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">data_utils.datasets</span> <span class="kn">import</span> <span class="n">get_data_loader</span><span class="p">,</span> <span class="n">DSET_NAME_TO_OBJECT</span>
    <span class="kn">from</span> <span class="nn">models.avit</span> <span class="kn">import</span> <span class="n">build_avit</span>
    <span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="n">logging_utils</span>
    <span class="kn">from</span> <span class="nn">utils.YParams</span> <span class="kn">import</span> <span class="n">YParams</span>
<span class="k">except</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">.data_utils.datasets</span> <span class="kn">import</span> <span class="n">get_data_loader</span><span class="p">,</span> <span class="n">DSET_NAME_TO_OBJECT</span>
    <span class="kn">from</span> <span class="nn">.models.avit</span> <span class="kn">import</span> <span class="n">build_avit</span>
    <span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="n">logging_utils</span>
    <span class="kn">from</span> <span class="nn">.utils.YParams</span> <span class="kn">import</span> <span class="n">YParams</span>


<div class="viewcode-block" id="add_weight_decay">
<a class="viewcode-back" href="../train_basic.html#train_basic.add_weight_decay">[docs]</a>
<span class="k">def</span> <span class="nf">add_weight_decay</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">inner_lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">skip_list</span><span class="o">=</span><span class="p">()):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; From Ross Wightman at:</span>
<span class="sd">        https://discuss.pytorch.org/t/weight-decay-in-the-optimizers-is-a-bad-idea-especially-with-batchnorm/16994/3 </span>
<span class="sd">        </span>
<span class="sd">        Goes through the parameter list and if the squeeze dim is 1 or 0 (usually means bias or scale) </span>
<span class="sd">        then don&#39;t apply weight decay. </span>
<span class="sd">        &quot;&quot;&quot;</span>
    <span class="n">decay</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">no_decay</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">skip_list</span><span class="p">):</span>
            <span class="n">no_decay</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">decay</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span>
            <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">no_decay</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.</span><span class="p">,},</span>
            <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">decay</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="n">weight_decay</span><span class="p">}]</span></div>


<div class="viewcode-block" id="Trainer">
<a class="viewcode-back" href="../train_basic.html#train_basic.Trainer">[docs]</a>
<span class="k">class</span> <span class="nc">Trainer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">global_rank</span><span class="p">,</span> <span class="n">local_rank</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">sweep_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">params</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">=</span> <span class="n">global_rank</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">=</span> <span class="n">local_rank</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;WORLD_SIZE&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sweep_id</span> <span class="o">=</span> <span class="n">sweep_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_to_screen</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">log_to_screen</span>
        <span class="c1"># Basic setup</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">startEpoch</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mp_type</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_bf16_supported</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">half</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">iters</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialize_data</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Initializing model on rank </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialize_model</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialize_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">resuming</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loading checkpoint </span><span class="si">%s</span><span class="s2">&quot;</span><span class="o">%</span><span class="n">params</span><span class="o">.</span><span class="n">checkpoint_path</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">restore_checkpoint</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">checkpoint_path</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">resuming</span> <span class="o">==</span> <span class="kc">False</span> <span class="ow">and</span> <span class="n">params</span><span class="o">.</span><span class="n">pretrained</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting from pretrained model at </span><span class="si">%s</span><span class="s2">&quot;</span><span class="o">%</span><span class="n">params</span><span class="o">.</span><span class="n">pretrained_ckpt_path</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">restore_checkpoint</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">pretrained_ckpt_path</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">iters</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">startEpoch</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># Do scheduler after checking for resume so we don&#39;t warmup every time</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialize_scheduler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>

<div class="viewcode-block" id="Trainer.single_print">
<a class="viewcode-back" href="../train_basic.html#train_basic.Trainer.single_print">[docs]</a>
    <span class="k">def</span> <span class="nf">single_print</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">text</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_to_screen</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">text</span><span class="p">]))</span></div>


<div class="viewcode-block" id="Trainer.initialize_data">
<a class="viewcode-back" href="../train_basic.html#train_basic.Trainer.initialize_data">[docs]</a>
    <span class="k">def</span> <span class="nf">initialize_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">tie_batches</span><span class="p">:</span>
            <span class="n">in_rank</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">in_rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_to_screen</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Initializing data on rank </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_data_loader</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_sampler</span> <span class="o">=</span> <span class="n">get_data_loader</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">params</span><span class="o">.</span><span class="n">train_data_paths</span><span class="p">,</span> 
                          <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">(),</span> <span class="n">split</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">in_rank</span><span class="p">,</span> <span class="n">train_offset</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">embedding_offset</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">valid_data_loader</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_data_loader</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">params</span><span class="o">.</span><span class="n">valid_data_paths</span><span class="p">,</span>
                                                                                 <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">(),</span>
                                                                        <span class="n">split</span><span class="o">=</span><span class="s1">&#39;val&#39;</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">in_rank</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_sampler</span><span class="o">.</span><span class="n">set_epoch</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span></div>



<div class="viewcode-block" id="Trainer.initialize_model">
<a class="viewcode-back" href="../train_basic.html#train_basic.Trainer.initialize_model">[docs]</a>
    <span class="k">def</span> <span class="nf">initialize_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s1">&#39;avit&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">build_avit</span><span class="p">(</span><span class="n">params</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">compile</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;WARNING: BFLOAT NOT SUPPORTED IN SOME COMPILE OPS SO SWITCHING TO FLOAT16&#39;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mp_type</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">half</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">DistributedDataParallel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">local_rank</span><span class="p">],</span>
                                                 <span class="n">output_device</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">local_rank</span><span class="p">],</span> <span class="n">find_unused_parameters</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">single_print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Model parameter count: </span><span class="si">{</span><span class="nb">sum</span><span class="p">([</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()])</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span></div>


<div class="viewcode-block" id="Trainer.initialize_optimizer">
<a class="viewcode-back" href="../train_basic.html#train_basic.Trainer.initialize_optimizer">[docs]</a>
    <span class="k">def</span> <span class="nf">initialize_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span> 
        <span class="n">parameters</span> <span class="o">=</span> <span class="n">add_weight_decay</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">)</span> <span class="c1"># Dont use weight decay on bias/scaling terms</span>
        <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;adam&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span>  <span class="n">DAdaptAdam</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">growth_rate</span><span class="o">=</span><span class="mf">1.05</span><span class="p">,</span> <span class="n">log_every</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">decouple</span><span class="o">=</span><span class="kc">True</span> <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">params</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;adan&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span>  <span class="n">DAdaptAdan</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">growth_rate</span><span class="o">=</span><span class="mf">1.05</span><span class="p">,</span> <span class="n">log_every</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adan</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">params</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;sgd&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span> 
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Optimizer </span><span class="si">{</span><span class="n">params</span><span class="o">.</span><span class="n">optimizer</span><span class="si">}</span><span class="s2"> not supported&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gscaler</span> <span class="o">=</span> <span class="n">amp</span><span class="o">.</span><span class="n">GradScaler</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mp_type</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">half</span> <span class="ow">and</span> <span class="n">params</span><span class="o">.</span><span class="n">enable_amp</span><span class="p">))</span></div>


<div class="viewcode-block" id="Trainer.initialize_scheduler">
<a class="viewcode-back" href="../train_basic.html#train_basic.Trainer.initialize_scheduler">[docs]</a>
    <span class="k">def</span> <span class="nf">initialize_scheduler</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">scheduler_epochs</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">sched_epochs</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">scheduler_epochs</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sched_epochs</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">max_epochs</span>
        <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">==</span> <span class="s1">&#39;cosine&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> 
                                                                            <span class="n">last_epoch</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">startEpoch</span><span class="o">*</span><span class="n">params</span><span class="o">.</span><span class="n">epoch_size</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
                                                                            <span class="n">T_max</span><span class="o">=</span><span class="n">sched_epochs</span><span class="o">*</span><span class="n">params</span><span class="o">.</span><span class="n">epoch_size</span><span class="p">,</span> 
                                                                            <span class="n">eta_min</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">/</span> <span class="mi">100</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">k</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">warmup_steps</span>
                <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">startEpoch</span><span class="o">*</span><span class="n">params</span><span class="o">.</span><span class="n">epoch_size</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">k</span><span class="p">:</span>
                    <span class="n">warmup</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">LinearLR</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">start_factor</span><span class="o">=</span><span class="mf">.01</span><span class="p">,</span> <span class="n">end_factor</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">total_iters</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
                    <span class="n">decay</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">eta_min</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">/</span> <span class="mi">100</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="n">sched_epochs</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">SequentialLR</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="p">[</span><span class="n">warmup</span><span class="p">,</span> <span class="n">decay</span><span class="p">],</span> <span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">last_epoch</span><span class="o">=</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">epoch_size</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">startEpoch</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="kc">None</span></div>



<div class="viewcode-block" id="Trainer.save_checkpoint">
<a class="viewcode-back" href="../train_basic.html#train_basic.Trainer.save_checkpoint">[docs]</a>
    <span class="k">def</span> <span class="nf">save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Save model and optimizer to checkpoint &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">model</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">({</span><span class="s1">&#39;iters&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">epoch_size</span><span class="p">,</span> <span class="s1">&#39;epoch&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> <span class="s1">&#39;model_state&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                    <span class="s1">&#39;optimizer_state_dict&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()},</span> <span class="n">checkpoint_path</span><span class="p">)</span></div>


<div class="viewcode-block" id="Trainer.restore_checkpoint">
<a class="viewcode-back" href="../train_basic.html#train_basic.Trainer.restore_checkpoint">[docs]</a>
    <span class="k">def</span> <span class="nf">restore_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Load model/opt from path &quot;&quot;&quot;</span>
        <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s1">&#39;cuda:</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">local_rank</span><span class="p">))</span>
        <span class="k">if</span> <span class="s1">&#39;model_state&#39;</span> <span class="ow">in</span> <span class="n">checkpoint</span><span class="p">:</span>
            <span class="n">model_state</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;model_state&#39;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">model_state</span> <span class="o">=</span> <span class="n">checkpoint</span>
        <span class="k">try</span><span class="p">:</span>  <span class="c1"># Try to load with DDP Wrapper</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">model_state</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>  <span class="c1"># If that fails, either try to load into module or strip DDP prefix</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">DDP</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">model_state</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_state_dict</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
                <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">model_state</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="c1"># Failing means this came from DDP - strip the DDP prefix</span>
                    <span class="n">name</span> <span class="o">=</span> <span class="n">key</span><span class="p">[</span><span class="mi">7</span><span class="p">:]</span>
                    <span class="n">new_state_dict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">new_state_dict</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">resuming</span><span class="p">:</span>  <span class="c1"># restore checkpoint is used for finetuning as well as resuming. If finetuning (i.e., not resuming), restore checkpoint does not load optimizer state, instead uses config specified lr.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">iters</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;iters&#39;</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;optimizer_state_dict&#39;</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">startEpoch</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;epoch&#39;</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">startEpoch</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">iters</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">pretrained</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">DDP</span><span class="p">):</span>
                <span class="n">model_to_modify</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">module</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">model_to_modify</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">freeze_middle</span><span class="p">:</span>
                <span class="n">model_to_modify</span><span class="o">.</span><span class="n">freeze_middle</span><span class="p">()</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">freeze_processor</span><span class="p">:</span>
                <span class="n">model_to_modify</span><span class="o">.</span><span class="n">freeze_processor</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">model_to_modify</span><span class="o">.</span><span class="n">unfreeze</span><span class="p">()</span>

            <span class="c1"># See how much we need to expand the projections</span>
            <span class="n">exp_proj</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="c1"># Iterate through the appended datasets and add on enough embeddings for all of them.</span>
            <span class="k">for</span> <span class="n">add_on</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">append_datasets</span><span class="p">:</span>
                <span class="n">exp_proj</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">DSET_NAME_TO_OBJECT</span><span class="p">[</span><span class="n">add_on</span><span class="p">]</span><span class="o">.</span><span class="n">_specifics</span><span class="p">()[</span><span class="mi">2</span><span class="p">])</span>
            <span class="n">model_to_modify</span><span class="o">.</span><span class="n">expand_projections</span><span class="p">(</span><span class="n">exp_proj</span><span class="p">)</span>

        <span class="n">checkpoint</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span></div>


<div class="viewcode-block" id="Trainer.train_one_epoch">
<a class="viewcode-back" href="../train_basic.html#train_basic.Trainer.train_one_epoch">[docs]</a>
    <span class="k">def</span> <span class="nf">train_one_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">tr_time</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">data_time</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">data_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">logs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;train_rmse&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
                <span class="s1">&#39;train_nrmse&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
            <span class="s1">&#39;train_l1&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)}</span>
        <span class="n">steps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">last_grads</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()]</span>
        <span class="n">grad_logs</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
        <span class="n">grad_counts</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
        <span class="n">loss_logs</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
        <span class="n">loss_counts</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">single_print</span><span class="p">(</span><span class="s1">&#39;train_loader_size&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_data_loader</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_data_loader</span><span class="p">):</span>
            <span class="n">steps</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">inp</span><span class="p">,</span> <span class="n">file_index</span><span class="p">,</span> <span class="n">field_labels</span><span class="p">,</span> <span class="n">bcs</span><span class="p">,</span> <span class="n">tar</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">data</span><span class="p">)</span> 
            <span class="n">dset_type</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">sub_dsets</span><span class="p">[</span><span class="n">file_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">type</span>
            <span class="n">loss_counts</span><span class="p">[</span><span class="n">dset_type</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">inp</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="s1">&#39;b t c h w -&gt; t b c h w&#39;</span><span class="p">)</span>
            <span class="n">data_time</span> <span class="o">+=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">data_start</span>
            <span class="n">dtime</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">data_start</span>
            
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">require_backward_grad_sync</span> <span class="o">=</span> <span class="p">((</span><span class="mi">1</span><span class="o">+</span><span class="n">batch_idx</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">accum_grad</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">enable_amp</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mp_type</span><span class="p">):</span>
                <span class="n">model_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">field_labels</span><span class="p">,</span> <span class="n">bcs</span><span class="p">)</span>
                <span class="n">spatial_dims</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">ndim</span><span class="p">))[</span><span class="mi">2</span><span class="p">:]</span> <span class="c1"># Assume 0, 1, 2 are T, B, C</span>
                <span class="n">residuals</span> <span class="o">=</span> <span class="n">output</span> <span class="o">-</span> <span class="n">tar</span>
                <span class="c1"># Differentiate between log and accumulation losses</span>
                <span class="n">tar_norm</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1e-7</span> <span class="o">+</span> <span class="n">tar</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">spatial_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
                <span class="n">raw_loss</span> <span class="o">=</span> <span class="p">((</span><span class="n">residuals</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">spatial_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
                         <span class="o">/</span> <span class="n">tar_norm</span><span class="p">)</span>
                <span class="c1"># Scale loss for accum</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">raw_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">accum_grad</span>
                <span class="n">forward_end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
                <span class="n">forward_time</span> <span class="o">=</span> <span class="n">forward_end</span><span class="o">-</span><span class="n">model_start</span>
                <span class="c1"># Logging</span>
                <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                    <span class="n">logs</span><span class="p">[</span><span class="s1">&#39;train_l1&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">F</span><span class="o">.</span><span class="n">l1_loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">tar</span><span class="p">)</span>
                    <span class="n">log_nrmse</span> <span class="o">=</span> <span class="n">raw_loss</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
                    <span class="n">logs</span><span class="p">[</span><span class="s1">&#39;train_nrmse&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">log_nrmse</span> <span class="c1"># ehh, not true nmse, but close enough</span>
                    <span class="n">loss_logs</span><span class="p">[</span><span class="n">dset_type</span><span class="p">]</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                    <span class="n">logs</span><span class="p">[</span><span class="s1">&#39;train_rmse&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">residuals</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">spatial_dims</span><span class="p">)</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
                <span class="c1"># Scaler is no op when not using AMP</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">gscaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                <span class="n">backward_end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
                <span class="n">backward_time</span> <span class="o">=</span> <span class="n">backward_end</span> <span class="o">-</span> <span class="n">forward_end</span>
                <span class="c1"># Only take step once per accumulation cycle</span>
                <span class="n">optimizer_step</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">require_backward_grad_sync</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">gscaler</span><span class="o">.</span><span class="n">unscale_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="mi">1</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">gscaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">gscaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                    <span class="n">optimizer_step</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">backward_end</span>
                <span class="n">tr_time</span> <span class="o">+=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">model_start</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_to_screen</span> <span class="ow">and</span> <span class="n">batch_idx</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">log_interval</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="si">}</span><span class="s2"> Batch </span><span class="si">{</span><span class="n">batch_idx</span><span class="si">}</span><span class="s2"> Train Loss </span><span class="si">{</span><span class="n">log_nrmse</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_to_screen</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Total Times. Batch: </span><span class="si">{}</span><span class="s1">, Rank: </span><span class="si">{}</span><span class="s1">, Data Shape: </span><span class="si">{}</span><span class="s1">, Data time: </span><span class="si">{}</span><span class="s1">, Forward: </span><span class="si">{}</span><span class="s1">, Backward: </span><span class="si">{}</span><span class="s1">, Optimizer: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">batch_idx</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtime</span><span class="p">,</span> <span class="n">forward_time</span><span class="p">,</span> <span class="n">backward_time</span><span class="p">,</span> <span class="n">optimizer_step</span><span class="p">))</span>
                <span class="n">data_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">logs</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">/</span><span class="n">steps</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">logs</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="c1"># If distributed, do lots of logging things</span>
        <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">logs</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
                <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">logs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span> 
                <span class="n">logs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">logs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">/</span><span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">())</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">loss_logs</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
                <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">loss_logs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">grad_logs</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
                <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">grad_logs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">loss_counts</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
                <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">loss_counts</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">grad_counts</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
                <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">grad_counts</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
            
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">loss_logs</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">logs</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s1">/train_nrmse&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss_logs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">/</span> <span class="n">loss_counts</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">iters</span> <span class="o">+=</span> <span class="n">steps</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logs</span><span class="p">[</span><span class="s1">&#39;iters&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">iters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">single_print</span><span class="p">(</span><span class="s1">&#39;all reduces executed!&#39;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">tr_time</span><span class="p">,</span> <span class="n">data_time</span><span class="p">,</span> <span class="n">logs</span></div>


<div class="viewcode-block" id="Trainer.validate_one_epoch">
<a class="viewcode-back" href="../train_basic.html#train_basic.Trainer.validate_one_epoch">[docs]</a>
    <span class="k">def</span> <span class="nf">validate_one_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">full</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Validates - for each batch just use a small subset to make it easier.</span>

<span class="sd">        Note: need to split datasets for meaningful metrics, but TBD. </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Don&#39;t bother with full validation set between epochs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">full</span><span class="p">:</span>
            <span class="n">cutoff</span> <span class="o">=</span> <span class="mi">999999999999</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">cutoff</span> <span class="o">=</span> <span class="mi">40</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">single_print</span><span class="p">(</span><span class="s1">&#39;STARTING VALIDATION!!!&#39;</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
            <span class="c1"># There&#39;s something weird going on when i turn this off.</span>
            <span class="k">with</span> <span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mp_type</span><span class="p">):</span>
                <span class="n">field_labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">valid_dataset</span><span class="o">.</span><span class="n">get_state_names</span><span class="p">()</span>
                <span class="n">distinct_dsets</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">([</span><span class="n">dset</span><span class="o">.</span><span class="n">title</span> <span class="k">for</span> <span class="n">dset_group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">valid_dataset</span><span class="o">.</span><span class="n">sub_dsets</span> 
                                           <span class="k">for</span> <span class="n">dset</span> <span class="ow">in</span> <span class="n">dset_group</span><span class="o">.</span><span class="n">get_per_file_dsets</span><span class="p">()]))</span>
                <span class="n">counts</span> <span class="o">=</span> <span class="p">{</span><span class="n">dset</span><span class="p">:</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">dset</span> <span class="ow">in</span> <span class="n">distinct_dsets</span><span class="p">}</span>
                <span class="n">logs</span> <span class="o">=</span> <span class="p">{}</span> <span class="c1"># </span>
                <span class="c1"># Iterate through all folder specific datasets</span>
                <span class="k">for</span> <span class="n">subset_group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">valid_dataset</span><span class="o">.</span><span class="n">sub_dsets</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">subset</span> <span class="ow">in</span> <span class="n">subset_group</span><span class="o">.</span><span class="n">get_per_file_dsets</span><span class="p">():</span>
                        <span class="n">dset_type</span> <span class="o">=</span> <span class="n">subset</span><span class="o">.</span><span class="n">title</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">single_print</span><span class="p">(</span><span class="s1">&#39;VALIDATING ON&#39;</span><span class="p">,</span> <span class="n">dset_type</span><span class="p">)</span>
                        <span class="c1"># Create data loader for each</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">use_ddp</span><span class="p">:</span>
                            <span class="n">temp_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">subset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
                                                                    <span class="n">num_workers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">num_data_workers</span><span class="p">,</span>
                                                                    <span class="n">sampler</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">DistributedSampler</span><span class="p">(</span><span class="n">subset</span><span class="p">,</span>
                                                                                                                            <span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                                    <span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="c1"># Seed isn&#39;t important, just trying to mix up samples from different trajectories</span>
                            <span class="n">temp_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">subset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
                                                                    <span class="n">num_workers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">num_data_workers</span><span class="p">,</span> 
                                                                    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
                                                                    <span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                        <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
                        <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">temp_loader</span><span class="p">):</span>
                            <span class="c1"># Only do a few batches of each dataset if not doing full validation</span>
                            <span class="k">if</span> <span class="n">count</span> <span class="o">&gt;</span> <span class="n">cutoff</span><span class="p">:</span>
                                <span class="k">del</span><span class="p">(</span><span class="n">temp_loader</span><span class="p">)</span>
                                <span class="k">break</span>
                            <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
                            <span class="n">counts</span><span class="p">[</span><span class="n">dset_type</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                            <span class="n">inp</span><span class="p">,</span> <span class="n">bcs</span><span class="p">,</span> <span class="n">tar</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">data</span><span class="p">)</span> 
                            <span class="c1"># Labels come from the trainset - useful to configure an extra field for validation sets not included</span>
                            <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">subset_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">subset</span><span class="o">.</span><span class="n">get_name</span><span class="p">(),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">valid_dataset</span><span class="o">.</span><span class="n">subset_dict</span><span class="p">[</span><span class="n">subset</span><span class="o">.</span><span class="n">get_name</span><span class="p">()])),</span>
                                                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">tar</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                            <span class="n">inp</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="s1">&#39;b t c h w -&gt; t b c h w&#39;</span><span class="p">)</span>
                            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">bcs</span><span class="p">)</span>
                            <span class="c1"># I don&#39;t think this is the true metric, but PDE bench averages spatial RMSE over batches (MRMSE?) rather than root after mean</span>
                            <span class="c1"># And we want the comparison to be consistent</span>
                            <span class="n">spatial_dims</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">ndim</span><span class="p">))[</span><span class="mi">2</span><span class="p">:]</span> <span class="c1"># Assume 0, 1, 2 are T, B, C</span>
                            <span class="n">residuals</span> <span class="o">=</span> <span class="n">output</span> <span class="o">-</span> <span class="n">tar</span>
                            <span class="n">nmse</span> <span class="o">=</span> <span class="p">(</span><span class="n">residuals</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">spatial_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
                                    <span class="o">/</span> <span class="p">(</span><span class="mf">1e-7</span> <span class="o">+</span> <span class="n">tar</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">spatial_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)))</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="c1">#.mean()</span>
                            <span class="n">logs</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">dset_type</span><span class="si">}</span><span class="s1">/valid_nrmse&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">logs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">dset_type</span><span class="si">}</span><span class="s1">/valid_nrmse&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">nmse</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
                            <span class="n">logs</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">dset_type</span><span class="si">}</span><span class="s1">/valid_rmse&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">logs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">dset_type</span><span class="si">}</span><span class="s1">/valid_mse&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span> 
                                                                <span class="o">+</span> <span class="n">residuals</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">spatial_dims</span><span class="p">)</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
                            <span class="n">logs</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">dset_type</span><span class="si">}</span><span class="s1">/valid_l1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">logs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">dset_type</span><span class="si">}</span><span class="s1">/valid_l1&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> 
                                                                <span class="o">+</span> <span class="n">residuals</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

                            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">field</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">valid_dataset</span><span class="o">.</span><span class="n">subset_dict</span><span class="p">[</span><span class="n">subset</span><span class="o">.</span><span class="n">type</span><span class="p">]):</span>
                                <span class="n">field_name</span> <span class="o">=</span> <span class="n">field_labels</span><span class="p">[</span><span class="n">field</span><span class="p">]</span>
                                <span class="n">logs</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">dset_type</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">field_name</span><span class="si">}</span><span class="s1">_valid_nrmse&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">logs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">dset_type</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">field_name</span><span class="si">}</span><span class="s1">_valid_nrmse&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> 
                                                                                <span class="o">+</span> <span class="n">nmse</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
                                <span class="n">logs</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">dset_type</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">field_name</span><span class="si">}</span><span class="s1">_valid_rmse&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">logs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">dset_type</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">field_name</span><span class="si">}</span><span class="s1">_valid_rmse&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> 
                                                                                    <span class="o">+</span> <span class="n">residuals</span><span class="p">[:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">spatial_dims</span><span class="p">)</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
                                <span class="n">logs</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">dset_type</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">field_name</span><span class="si">}</span><span class="s1">_valid_l1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">logs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">dset_type</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">field_name</span><span class="si">}</span><span class="s1">_valid_l1&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> 
                                                                            <span class="o">+</span>  <span class="n">residuals</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="k">del</span><span class="p">(</span><span class="n">temp_loader</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">single_print</span><span class="p">(</span><span class="s1">&#39;DONE VALIDATING - NOW SYNCING&#39;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">logs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">dset_type</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">logs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="o">/</span><span class="n">counts</span><span class="p">[</span><span class="n">dset_type</span><span class="p">]</span>

            <span class="n">logs</span><span class="p">[</span><span class="s1">&#39;valid_nrmse&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">dset_type</span> <span class="ow">in</span> <span class="n">distinct_dsets</span><span class="p">:</span>
                <span class="n">logs</span><span class="p">[</span><span class="s1">&#39;valid_nrmse&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">logs</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">dset_type</span><span class="si">}</span><span class="s1">/valid_nrmse&#39;</span><span class="p">]</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">distinct_dsets</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">logs</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
                    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">logs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span> <span class="c1"># There was a bug with means when I implemented this - dont know if fixed</span>
                    <span class="n">logs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">logs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="o">/</span><span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">())</span>
                    <span class="k">if</span> <span class="s1">&#39;rmse&#39;</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
                        <span class="n">logs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">logs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">single_print</span><span class="p">(</span><span class="s1">&#39;DONE SYNCING - NOW LOGGING&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logs</span>               </div>



<div class="viewcode-block" id="Trainer.train">
<a class="viewcode-back" href="../train_basic.html#train_basic.Trainer.train">[docs]</a>
    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># This is set up this way based on old code to allow wandb sweeps</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">log_to_wandb</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sweep_id</span><span class="p">:</span>
                <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="nb">dir</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">experiment_dir</span><span class="p">)</span>
                <span class="n">hpo_config</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">as_dict</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">update_params</span><span class="p">(</span><span class="n">hpo_config</span><span class="p">)</span>
                <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="nb">dir</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">experiment_dir</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">group</span><span class="p">,</span> 
                           <span class="n">project</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">project</span><span class="p">,</span> <span class="n">entity</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">entity</span><span class="p">,</span> <span class="n">resume</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sweep_id</span> <span class="ow">and</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
            <span class="n">param_file</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;temp_hpo_config_</span><span class="si">{</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;SLURM_JOBID&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">.pkl&quot;</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">param_file</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                    <span class="n">pkl</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">hpo_config</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span> <span class="c1"># Stop until the configs are written by hacky MPI sub</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span> 
                <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">param_file</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                    <span class="n">hpo_config</span> <span class="o">=</span> <span class="n">pkl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span> <span class="c1"># Stop until the configs are written by hacky MPI sub</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">param_file</span><span class="p">)</span>
            <span class="c1"># If tuning batch size, need to go from global to local batch size</span>
            <span class="k">if</span> <span class="s1">&#39;batch_size&#39;</span> <span class="ow">in</span> <span class="n">hpo_config</span><span class="p">:</span>
                <span class="n">hpo_config</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">hpo_config</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">update_params</span><span class="p">(</span><span class="n">hpo_config</span><span class="p">)</span>
            <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">initialize_data</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span> <span class="c1"># This is the annoying redundant part - but the HPs need to be set from wandb</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">initialize_model</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span> 
            <span class="bp">self</span><span class="o">.</span><span class="n">initialize_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">initialize_scheduler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">summary</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">log_to_wandb</span><span class="p">:</span>
            <span class="n">wandb</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">single_print</span><span class="p">(</span><span class="s2">&quot;Starting Training Loop...&quot;</span><span class="p">)</span>
        <span class="c1"># Actually train now, saving checkpoints, logging time, and logging to wandb</span>
        <span class="n">best_valid_loss</span> <span class="o">=</span> <span class="mf">1.e6</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">startEpoch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">max_epochs</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train_sampler</span><span class="o">.</span><span class="n">set_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

            <span class="c1"># with torch.autograd.detect_anomaly(check_nan=True):</span>
            <span class="n">tr_time</span><span class="p">,</span> <span class="n">data_time</span><span class="p">,</span> <span class="n">train_logs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_one_epoch</span><span class="p">()</span>
            
            <span class="n">valid_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
            <span class="c1"># Only do full validation set on last epoch - don&#39;t waste time</span>
            <span class="k">if</span> <span class="n">epoch</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">max_epochs</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="n">valid_logs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">validate_one_epoch</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">valid_logs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">validate_one_epoch</span><span class="p">()</span>
            
            <span class="n">post_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
            <span class="n">train_logs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">valid_logs</span><span class="p">)</span>
            <span class="n">train_logs</span><span class="p">[</span><span class="s1">&#39;time/train_time&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">valid_start</span><span class="o">-</span><span class="n">start</span>
            <span class="n">train_logs</span><span class="p">[</span><span class="s1">&#39;time/train_data_time&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data_time</span>
            <span class="n">train_logs</span><span class="p">[</span><span class="s1">&#39;time/train_compute_time&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tr_time</span>
            <span class="n">train_logs</span><span class="p">[</span><span class="s1">&#39;time/valid_time&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">post_start</span><span class="o">-</span><span class="n">valid_start</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">log_to_wandb</span><span class="p">:</span>
                <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">train_logs</span><span class="p">)</span> 
            <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">checkpoint_path</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">checkpoint_save_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">checkpoint_path</span> <span class="o">+</span> <span class="sa">f</span><span class="s1">&#39;_epoch</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">valid_logs</span><span class="p">[</span><span class="s1">&#39;valid_nrmse&#39;</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">best_valid_loss</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">best_checkpoint_path</span><span class="p">)</span>
                    <span class="n">best_valid_loss</span> <span class="o">=</span> <span class="n">valid_logs</span><span class="p">[</span><span class="s1">&#39;valid_nrmse&#39;</span><span class="p">]</span>
                
                <span class="n">cur_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">single_print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Time for train </span><span class="si">{</span><span class="n">valid_start</span><span class="o">-</span><span class="n">start</span><span class="si">}</span><span class="s1">. For valid: </span><span class="si">{</span><span class="n">post_start</span><span class="o">-</span><span class="n">valid_start</span><span class="si">}</span><span class="s1">. For postprocessing:</span><span class="si">{</span><span class="n">cur_time</span><span class="o">-</span><span class="n">post_start</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">single_print</span><span class="p">(</span><span class="s1">&#39;Time taken for epoch </span><span class="si">{}</span><span class="s1"> is </span><span class="si">{}</span><span class="s1"> sec&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">single_print</span><span class="p">(</span><span class="s1">&#39;Train loss: </span><span class="si">{}</span><span class="s1">. Valid loss: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_logs</span><span class="p">[</span><span class="s1">&#39;train_nrmse&#39;</span><span class="p">],</span> <span class="n">valid_logs</span><span class="p">[</span><span class="s1">&#39;valid_nrmse&#39;</span><span class="p">]))</span></div>
</div>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--run_name&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;00&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--use_ddp&quot;</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="s1">&#39;store_true&#39;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Use distributed data parallel&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--yaml_config&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;./config/mpp_avit_s_config.yaml&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--config&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;frozen&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--sweep_id&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;sweep config from ./configs/sweeps.yaml&#39;</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">YParams</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">yaml_config</span><span class="p">),</span> <span class="n">args</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
    <span class="n">params</span><span class="o">.</span><span class="n">use_ddp</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">use_ddp</span>
    <span class="c1"># Set up distributed training</span>
    <span class="n">local_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;LOCAL_RANK&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">global_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;RANK&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;WORLD_SIZE&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">use_ddp</span><span class="p">:</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="s2">&quot;nccl&quot;</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span> <span class="c1"># Torch docs recommend just using device, but I had weird memory issues without setting this.</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

    <span class="c1"># Modify params</span>
    <span class="n">params</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">batch_size</span><span class="o">//</span><span class="n">world_size</span><span class="p">)</span>
    <span class="n">params</span><span class="p">[</span><span class="s1">&#39;startEpoch&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">sweep_id</span><span class="p">:</span>
        <span class="n">jid</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;SLURM_JOBID&#39;</span><span class="p">]</span> <span class="c1"># so different sweeps dont resume</span>
        <span class="n">expDir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">exp_dir</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">sweep_id</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">run_name</span><span class="p">),</span> <span class="n">jid</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">expDir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">exp_dir</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">run_name</span><span class="p">))</span>

    <span class="n">params</span><span class="p">[</span><span class="s1">&#39;old_exp_dir&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">expDir</span> <span class="c1"># I dont remember what this was for but not removing it yet</span>
    <span class="n">params</span><span class="p">[</span><span class="s1">&#39;experiment_dir&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">expDir</span><span class="p">)</span>
    <span class="n">params</span><span class="p">[</span><span class="s1">&#39;checkpoint_path&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">expDir</span><span class="p">,</span> <span class="s1">&#39;training_checkpoints/ckpt.tar&#39;</span><span class="p">)</span>
    <span class="n">params</span><span class="p">[</span><span class="s1">&#39;best_checkpoint_path&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">expDir</span><span class="p">,</span> <span class="s1">&#39;training_checkpoints/best_ckpt.tar&#39;</span><span class="p">)</span>
    <span class="n">params</span><span class="p">[</span><span class="s1">&#39;old_checkpoint_path&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">old_exp_dir</span><span class="p">,</span> <span class="s1">&#39;training_checkpoints/best_ckpt.tar&#39;</span><span class="p">)</span>
    
    <span class="c1"># Have rank 0 check for and/or make directory</span>
    <span class="k">if</span>  <span class="n">global_rank</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">expDir</span><span class="p">):</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">expDir</span><span class="p">)</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">expDir</span><span class="p">,</span> <span class="s1">&#39;training_checkpoints/&#39;</span><span class="p">))</span>
    <span class="n">params</span><span class="p">[</span><span class="s1">&#39;resuming&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span> <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">checkpoint_path</span><span class="p">)</span> <span class="k">else</span> <span class="kc">False</span>

    <span class="c1"># WANDB things</span>
    <span class="n">params</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span> <span class="o">=</span>  <span class="nb">str</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">run_name</span><span class="p">)</span>
    <span class="c1"># params[&#39;group&#39;] = params[&#39;group&#39;] #+ args.config</span>
    <span class="c1"># params[&#39;project&#39;] = &quot;pde_bench&quot;</span>
    <span class="c1"># params[&#39;entity&#39;] = &quot;flatiron-scipt&quot;</span>
    <span class="k">if</span> <span class="n">global_rank</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
        <span class="n">logging_utils</span><span class="o">.</span><span class="n">log_to_file</span><span class="p">(</span><span class="n">logger_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">log_filename</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">expDir</span><span class="p">,</span> <span class="s1">&#39;out.log&#39;</span><span class="p">))</span>
        <span class="n">logging_utils</span><span class="o">.</span><span class="n">log_versions</span><span class="p">()</span>
        <span class="n">params</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>


    <span class="n">params</span><span class="p">[</span><span class="s1">&#39;log_to_wandb&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">global_rank</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span> <span class="ow">and</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;log_to_wandb&#39;</span><span class="p">]</span>
    <span class="n">params</span><span class="p">[</span><span class="s1">&#39;log_to_screen&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">global_rank</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span> <span class="ow">and</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;log_to_screen&#39;</span><span class="p">]</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="n">global_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">hparams</span> <span class="o">=</span> <span class="n">ruamelDict</span><span class="p">()</span>
        <span class="n">yaml</span> <span class="o">=</span> <span class="n">YAML</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">hparams</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">key</span><span class="p">)]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">expDir</span><span class="p">,</span> <span class="s1">&#39;hyperparams.yaml&#39;</span><span class="p">),</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">hpfile</span><span class="p">:</span>
            <span class="n">yaml</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">hparams</span><span class="p">,</span>  <span class="n">hpfile</span> <span class="p">)</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">global_rank</span><span class="p">,</span> <span class="n">local_rank</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">sweep_id</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">sweep_id</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">sweep_id</span> <span class="ow">and</span> <span class="n">trainer</span><span class="o">.</span><span class="n">global_rank</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">sweep_id</span><span class="p">,</span> <span class="n">trainer</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">entity</span><span class="p">,</span> <span class="n">trainer</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">project</span><span class="p">)</span>
        <span class="n">wandb</span><span class="o">.</span><span class="n">agent</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">sweep_id</span><span class="p">,</span> <span class="n">function</span><span class="o">=</span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">,</span> <span class="n">count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">entity</span><span class="o">=</span><span class="n">trainer</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">entity</span><span class="p">,</span> <span class="n">project</span><span class="o">=</span><span class="n">trainer</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">project</span><span class="p">)</span> 
    <span class="k">else</span><span class="p">:</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">log_to_screen</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;DONE ---- rank </span><span class="si">%d</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">global_rank</span><span class="p">)</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>